# -*- coding: utf-8 -*-
"""final eng-hindi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AUzCaSORU778H7oZutYal3lM9zHng-Oi
"""

!pip install datasets transformers[sentencepiece] sacrebleu -q

import os
import sys
import transformers
import tensorflow as tf
#from datasets import load_dataset
from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq
from transformers import AdamWeightDecay
from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM

model_checkpoint = "Helsinki-NLP/opus-mt-en-hi"

raw_datasets = load_dataset("cfilt/iitb-english-hindi")

raw_datasets

raw_datasets['train'][1:200000]

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)

tokenizer("Accerciser could not see the applications on your desktop. You must enable desktop accessibility to fix this problem. Do you want to enable it now")

tokenizer(["The duration of the highlight box when selecting accessible nodes"])

with tokenizer.as_target_tokenizer():
    print(tokenizer(["प्लग-इन जिसमें हैं एक्सेसेबेलों को तेजी से चुनने के लिए कई विधियां"]))

max_input_length = 128
max_target_length = 128

source_lang = "en"
target_lang = "hi"


def preprocess_function(examples):
    inputs = [ex[source_lang] for ex in examples["translation"]]
    targets = [ex[target_lang] for ex in examples["translation"]]
    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

    # Setup the tokenizer for targets
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(targets, max_length=max_target_length, truncation=True)

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

preprocess_function(raw_datasets["train"][:10])

tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

model = TFAutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)

batch_size = 16
learning_rate = 2e-5
weight_decay = 0.01
num_train_epochs = 1

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf")

generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors="tf", pad_to_multiple_of=128)

train_dataset = model.prepare_tf_dataset(
    tokenized_datasets["test"],
    batch_size=batch_size,
    shuffle=True,
    collate_fn=data_collator,
)

validation_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    batch_size=batch_size,
    shuffle=False,
    collate_fn=data_collator,
)

generation_dataset = model.prepare_tf_dataset(
    tokenized_datasets["validation"],
    batch_size=8,
    shuffle=False,
    collate_fn=generation_data_collator,
)

optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)
model.compile(optimizer=optimizer)

model.fit(train_dataset, validation_data=validation_dataset, epochs=10)

model.save_pretrained("tf_model/")

tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
model = TFAutoModelForSeq2SeqLM.from_pretrained("tf_model/")

input_text  = "what is the architecture of this model"

tokenized = tokenizer([input_text], return_tensors='np')
out = model.generate(**tokenized, max_length=128)
print(out)

with tokenizer.as_target_tokenizer():
    print(tokenizer.decode(out[0], skip_special_tokens=True))

#!pip install ipywidgets

'''import ipywidgets as widgets
from IPython.display import display

# Create text input for English sentence
english_input = widgets.Textarea(
    placeholder='Enter English text here',
    description='English:',
    layout=widgets.Layout(width='100%', height='100px')
)

# Create a button to trigger the translation
translate_button = widgets.Button(
    description='Translate',
    button_style='success',
    layout=widgets.Layout(width='100px')
)

# Create an output widget to display the Hindi translation
hindi_output = widgets.Textarea(
    description='Hindi:',
    layout=widgets.Layout(width='100%', height='100px'),
    disabled=True
)
def examples(text):
  tokenized = tokenizer([text], return_tensors='np')
  out = model.generate(**tokenized, max_length=128)
  with tokenizer.as_target_tokenizer():
     translated_text = tokenizer.decode(out[0], skip_special_tokens=True)
  return translated_text

# Function to perform the translation and display result
def on_translate_button_clicked(b):
    english_text = english_input.value
    hindi_text = examples(english_text)  # Call your actual translation function here
    hindi_output.value = hindi_text

# Set button click handler
translate_button.on_click(on_translate_button_clicked)

# Display the widgets in Colab
display(english_input)
display(translate_button)
display(hindi_output)'''